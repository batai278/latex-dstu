\section{Теоретичні відомості}
\subsection{Метод головних компонент}
Метод головних компонент (Principal component analysis) --- метод, що дозволяє
зменшити розмірність досліджуваної вибірки з мінімальними втратами інформації.

Маємо $m$ об’єктів, з яких треба зняти по $n$ певних властивостей.
На вході в нас є виборки $\vec{X}_k$, кожна з яких відповідає сукупності
властивостей $k$-го об’єкту
\begin{equation*}
  \vec{X}_k = \begin{bmatrix}
    x_k^1  \\
    x_k^2  \\
    \vdots \\
    x_k^n
  \end{bmatrix},
  \qquad k = \overline{1,m}
\end{equation*}
Згрупуємо всі вимірювання в одну матрицю $X$
\begin{equation*}
  X = \begin{bmatrix}
    x_1^1  & x_2^1  & \dots  & x_m^1  \\
    x_1^2  & x_2^2  & \dots  & x_m^2  \\
    \vdots & \vdots & \ddots & \vdots \\
    x_1^n  & x_2^n  & \dots  & x_m^n
  \end{bmatrix}
\end{equation*}

Спочатку нам знадобиться знайти вибіркові середні значення для кожної
властивості
\begin{equation*}
  a_i = \frac{1}{m} \cdot \sum_{k=1}^{m} x_k^i, \qquad i = \overline{1,n}
\end{equation*}
Маємо вектор вибіркових середніх значень
\begin{comment}
\begin{equation*}
  \vec{a} = \begin{bmatrix}
    \frac{1}{m} \sum_{k=1}^{m} x_k^1 \\
    \frac{1}{m} \sum_{k=1}^{m} x_k^2 \\
    \vdots                           \\
    \frac{1}{m} \sum_{k=1}^{m} x_k^n \\
  \end{bmatrix}
\end{equation*}
\end{comment}
\begin{equation*}
  \vec{a} = \begin{bmatrix}
    a_1    \\
    a_2    \\
    \vdots \\
    a_n
  \end{bmatrix}
\end{equation*}

Центруємо отримані дані, що містяться в матриці $X$, віднявши від кожного
стовбця вектор вибіркових середніх $\vec{a}$
\begin{comment}
\begin{equation*}
  \tilde{x}_k^i = x_k^i - a_i,\qquad k = \overline{1,m}, i = \overline{1,n}
\end{equation*}
Кожен стовбець матриці вимірювань прийме вигляд
\begin{equation*}
  \tilde{X}_k
  = \vec{X}_k - \vec{a}
  = \begin{bmatrix}
    x_k^1 - a_1 \\
    x_k^2 - a_2 \\
    \vdots      \\
    x_k^n - a_n
  \end{bmatrix},
  \qquad k = \overline{1,m}
\end{equation*}
Маємо матрицю центрованих вимірювань $\tilde{X}$
\end{comment}
\begin{equation*}
  \tilde{X}
  = \begin{bmatrix}
    \tilde{x}_1^1  & \tilde{x}_2^1  & \dots  & \tilde{x}_m^1  \\
    \tilde{x}_1^2  & \tilde{x}_2^2  & \dots  & \tilde{x}_m^2  \\
    \vdots & \vdots & \ddots & \vdots \\
    \tilde{x}_1^n  & \tilde{x}_2^n  & \dots  & \tilde{x}_m^n
  \end{bmatrix}
  = \begin{bmatrix}
    x_1^1 - a_1  & x_2^1 - a_1  & \dots  & x_m^1 - a_1 \\
    x_1^2 - a_2  & x_2^2 - a_2  & \dots  & x_m^2 - a_2 \\
    \vdots       & \vdots       & \ddots & \vdots      \\
    x_1^n - a_n  & x_2^n - a_n  & \dots  & x_m^n - a_n
  \end{bmatrix}
\end{equation*}

Обчислюємо вибіркову коваріаційну матрицю властивостей.
Вибіркову коваріацію $i$ та $j$ властивості рахуємо за формулою
\begin{equation*}
  \sigma_i^j
  = \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^i \cdot \tilde{x}_k^j
  = \frac{1}{m} \cdot \sum_{k=1}^{m}
    \left[ \left( x_k^i - a_i \right) \cdot \left( x_k^j - a_j \right) \right],
    \qquad i,j = \overline{1,n}
\end{equation*}
Маємо вибіркову коваріаційну матрицю
\begin{equation*}
  K = \begin{bmatrix}
    \sigma_1^1 & \sigma_2^1 & \dots  & \sigma_n^1 \\
    \sigma_1^2 & \sigma_2^2 & \dots  & \sigma_n^2 \\
    \vdots     & \vdots     & \ddots & \vdots     \\
    \sigma_1^n & \sigma_2^n & \dots  & \sigma_n^n \\
  \end{bmatrix}
\end{equation*}
\begin{comment}
\begin{equation*}
  K =
  \begin{bmatrix}
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^1 \cdot \tilde{x}_k^1
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^1 \cdot \tilde{x}_k^2
    &
    \dots
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^1 \cdot \tilde{x}_k^n
    \\
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^2 \cdot \tilde{x}_k^1
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^2 \cdot \tilde{x}_k^2
    &
    \dots
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^2 \cdot \tilde{x}_k^n
    \\
    \vdots & \vdots & \dots & \vdots \\
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^n \cdot \tilde{x}_k^1
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^n \cdot \tilde{x}_k^2
    &
    \dots
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^n \cdot \tilde{x}_k^n
  \end{bmatrix}
\end{equation*}
\end{comment}

Щоб отримувати лише потрібну інформацію, ми хочемо знайти таке ортогональне
лінійне перетворення $L$ вхідної матриці $\tilde{X}$, щоб отримати матрицю
$Y = L \cdot \tilde{X}$, яка має діагональну вибіркову ковариаційну матрицю $K'$
з незростаючими зверху вниз значеннями.
Діагональна вибіркова коваріаційна матриця гарантує той факт, що отримані
значення $Y$ будуть некорельованими.
Рангування значень діагональних елементів матриці $K'$ за величиною дасть більш
наглядне представлення про будову досліджуваних об’єктів, адже діагональні
елементи --- вибіркові дисперсії; а чим більше дисперсія, тим більше відповідна
властивість змінюється від об’єкту до об’єкту і тим більше корисної інформації
вона нам надає.

Вибіркова коваріаційна матриця $K'$ для $Y = L \cdot \tilde{X}$ має вигляд
\begin{equation*}
  K'
  = L \cdot K \cdot L^*
  = \begin{bmatrix}
    \lambda_1 & 0         & \dots  & 0      \\
    0         & \lambda_2 & \dots  & 0      \\
    \vdots    & \vdots    & \ddots & \vdots \\
    0         & 0         & \dots  & \lambda_n
  \end{bmatrix}
\end{equation*}
З лінійної алгебри відомо, що матриця $L$ складається з координат власних
векторів матриці $K$, а елементи $\lambda_k$ --- її власні числа, які
існують і є невід’ємними через невід’ємну означеність матриці $K$.
Вважаємо що числа $\lambda_1, \dots, \lambda_n$ впорядковані від більшого до
меншого для зручності подальших дій.
Позначимо власний вектор матриці $K$, що відповідає власному числу $\lambda_k$,
як $\vec{l}_k$. Тоді
\begin{equation*}
  \vec{l}_k
  = \left[ l_k^1, l_k^2, \dots, l_k^n \right],
  \qquad k = \overline{1,n}
\end{equation*}
Матриця $L$ має вигляд
\begin{equation*}
  L = \begin{bmatrix}
    l_1^1  & l_1^2  & \dots  & l_1^n  \\
    l_2^1  & l_2^2  & \dots  & l_2^n  \\
    \vdots & \vdots & \ddots & \vdots \\
    l_n^1  & l_n^2  & \dots  & l_n^n  \\
  \end{bmatrix}
\end{equation*}

Треба зменшити розмірність простору досліджуваних параметрів системи з $n$ до
$p<n$, але при цьому втратити якомога менше відомостей про досліджувані
об’єкти.
Введемо міру інформації, що залишається при зменшенні кількості компонент, що
розглядаються
\begin{equation*}
  I = \frac{\lambda_1 + \dots + \lambda_p}{\lambda_1 + \dots + \lambda_n}
\end{equation*}
Будемо вважати, що діємо продуктивно, тому починаємо обирати з перших
компонент, адже саме вони є найбільш інформативними.
Також бачимо, що інформативність змінюється в межах від $0$
(нічого не дізнаємось) до $1$ (зберегли усю інформацію).

Надалі буде розглядатися матриця головних компонент $Y$
\begin{equation*}
  Y = \begin{bmatrix}
    y_1^1  & y_2^1  & \dots  & y_m^1  \\
    y_1^2  & y_2^2  & \dots  & y_m^2  \\
    \vdots & \vdots & \ddots & \vdots \\
    y_1^p  & y_2^p  & \dots  & y_m^p  \\
    \end{bmatrix}
\end{equation*}
\subsection{Гістограма}

Для подальшого аналізу потрібно здобути щільність розподілу головних компонент.
Оскільки маємо справу з вибіркою і вибірковими характеристиками,
потрібно побудувати гістограму, адже це і є вибіркова характеристика,
що відповідає щільності.

Побудуємо $j$-й стовбець гістограми для виборки з $k$-ї строки матриці $Y$
\begin{equation*}
  h_j^k = \frac{1}{m} \cdot \sum_{i=1}^{m} \indicator{y_i^k \in I_j^k},
  \qquad j = \overline{1, N},
  \qquad k = \overline{1, p}
\end{equation*}
де $I^k$ --- набір напівінтервалів, що розбиває відрізок
$\left[ \min\limits_{i=\overline{1,m}}{y_i^k};
\max\limits_{i=\overline{1,m}}{y_i^k} \right]$ на $N$ рівних частин.
Для вибору $N$ можна скористатися досить відомою формулою Стьорджеса
(Sturges' formula) \cite{Sturges:1926:CCI}
\begin{equation*}
  N = \lfloor \log_2 m \rfloor + 1
\end{equation*}
Маємо матрицю гістограм
\begin{equation*}
  H = \begin{bmatrix}
    h_1^1  & h_2^1  & \dots  & h_N^1  \\
    h_1^2  & h_2^2  & \dots  & h_N^2  \\
    \vdots & \vdots & \ddots & \vdots \\
    h_1^p  & h_2^p  & \dots  & h_N^p
  \end{bmatrix}
\end{equation*}
і напівінтервалів, що відповідають кожному стовбчику кожної гістограми
\begin{equation*}
  I = \begin{bmatrix}
    I_1^1  & I_2^1  & \dots  & I_N^1  \\
    I_1^2  & I_2^2  & \dots  & I_N^2  \\
    \vdots & \vdots & \ddots & \vdots \\
    I_1^p  & I_2^p  & \dots  & I_N^p
  \end{bmatrix}
\end{equation*}

\subsection{Поліноміальний розподіл}
Введемо матрицю частот $\nu$
\begin{equation*}
  \nu = p \cdot H
\end{equation*}
Кожна компонента --- кількість елементів вибірки, що потрапили у відповідний
напівінтервал
\begin{equation*}
  \nu_j^k = \sum_{i=1}^{m} \indicator{y_i^k \in I_j^k},
  \qquad j = \overline{1, N},
  \qquad k = \overline{1, p}
\end{equation*}

Розглянемо вектор
\begin{equation*}
  \nu^k = \left[ \nu_1^k, \dots, \nu_N^k \right]
\end{equation*}
Маємо серію з $m$ незалежних експериментів, кожен з яких може закінчитися
одним з $N$ результатів $E_1^k$, $\dots$, $E_N^k$.
Якщо випадкові величини $y_i^k$ мають заздалегіть відомий
розподіл, який однаковий в межах однієї строки $Y^k$, маємо ймовірності
кожного результату експерименту
\begin{equation*}
  \rho_j^k = \probability{E_j^k} = \probability{y_1^k \in I_j^k}
  = \dots = \probability{y_m^k \in I_j^k},
  \qquad j = \overline{1,N},
  \qquad k = \overline{1,p}
\end{equation*}

Математичне сподівання і дисперсія кожного елементу співпадає з математичним
сподіванням і дисперсією біноміального розподілу з відповідними
характеристиками, адже випадкові величини $\nu_j^k$ не залежать одна від одної
\begin{equation*}
  \mean{\nu_j^k} = N \cdot \rho_j^k,\qquad
  \dispersion{\nu_j^k} = N \cdot \rho_j^k \cdot \left( 1 - \rho_j^k \right),
  \qquad j = \overline{1,N},
  \qquad k = \overline{1,p}
\end{equation*}
Коваріація двох різних елементів вектора $\nu^k$ рахується за формулою
\cite{Mukhopadhyay:2000}
\begin{equation*}
  \cov{\nu_j^k, \nu_i^k} = - N \cdot \rho_j^k \cdot \rho_i^k,
  \qquad i \neq j
  %\qquad k = \overline{1,p},
  %\qquad i,j = \overline{1,N},
\end{equation*}
Отже, коваріаційна матриця $A$ вектора $\nu^k$ виглядає наступним чином
\begin{equation*}
  A^k = N \cdot
   \begin{bmatrix}
      \rho_1^k \cdot \left( 1 - \rho_1^k \right) & - \rho_1^k \cdot \rho_2^k
              & \cdots & - \rho_1^k \cdot \rho_N^k \\

      - \rho_2^k \cdot \rho_1^k                  & \rho_2^k \cdot
          \left( 1 - \rho_2^k \right) & \cdots & - \rho_2^k \cdot \rho_N^k \\

      \vdots                                     & \vdots
              & \ddots & \vdots \\

       - \rho_N^k \cdot \rho_1^k                 & - \rho_N^k \cdot \rho_2^k
              & \cdots & \rho_N^k \cdot \left( 1 - \rho_N^k \right)
    \end{bmatrix}
\end{equation*}

\subsection{Критерій узгодженості Пірсона $\chi^2$}
Гістограма може використовуватися не тільки для графічної інтерпретації
отриманих даних, але й для віднесення вибірки до якогось відомого розподілу.
Відповідь на питання ``Чи дійсно вибірка $y_1^k$, $\dots$, $y_p^k$ має розподіл
$F^k$?'' може надати критерій узгодженості Пірсона. %\cite{Pearson:1900}.

Розглянемо вектор
\begin{equation*}
  S^k
  = \left[ p \cdot h_1^k, \dots, p \cdot h_N^k \right]
  = \left[ \sum_{i=1}^{p} \indicator{y_i^k \in I_1^k}, \dots,
    \sum_{i=1}^{p} \indicator{y_i^k \in I_N^k} \right]
\end{equation*}
Кожна компонента є сумою $p$ результатів бернулієвських експериментів,
ймовірність успіху якого заздалегіть невідома --- саме ця характеристика й
визначається припущенням щодо розподілу.
Отже, треба визначити ймовірність $\rho_i^k$ того, що випадкова величина
$\xi^k$ з функцією розподілу $F^k$ потрапить у напівінтервал $I_i^k$
\begin{equation*}
  \rho_i^k = \probability{\xi^k \in I_i^k},
  \qquad \probability{\xi^k \le x} = \cdfof{k}{x},
  \qquad k = \overline{1,p}
\end{equation*}
Тобто вектор $S$ має поліноміальний розподіл --- розподіл експерименту, що
складається з $p$ випробувань, кожне з яких може мати лише один з $N$
результатів $E_1$, $\dots$, $E_N$ \cite{Cramer:1999}.
Кількість випробувань з результатом $E_i$ знаходиться в $i$-ій компоненті
вектора, а сума всіх компонент дорівнює $p$.

Згідно з багатовимірною центральною граничною теоремою маємо
\begin{equation*}
  S^k
  \sim N\left( \left[ p \cdot \rho_1^k, \dots, p \cdot \rho_N^k \right],
    A \right),
  \qquad p\to\infty
\end{equation*}
Коваріаційна матриця $A$ визначається наступним чином
\begin{equation*}
  A = p \cdot
    \begin{bmatrix}
      \rho_1^k \cdot \left( 1 - \rho_1^k \right) & - \rho_1^k \cdot \rho_2^k
                                      & \cdots & - \rho_1^k \cdot \rho_N^k \\
      - \rho_2^k \cdot \rho_1^k                  & \rho_2^k \cdot
          \left( 1 - \rho_2^k \right) & \cdots & - \rho_2^k \cdot \rho_N^k \\
      %- \rho_3^k \cdot \rho_1^k                  & - \rho_3^k \cdot \rho_2^k
      %                                & \cdots & - \rho_2^k \cdot \rho_N^k \\
      \vdots                                     & \vdots
                                      & \ddots & \vdots \\
       - \rho_N^k \cdot \rho_1^k                 & - \rho_N^k \cdot \rho_2^k
                                      & \cdots & \rho_N^k \cdot \left( 1 - \rho_N^k \right)
    \end{bmatrix}
\end{equation*}
Відомо, що 
\begin{equation*}
  \sum_{i=1}^{N} h_i^k = 1
\end{equation*}
Це означає, що матриця $A$ є виродженою. Тобто, ми не можемо знайти зворотню до
неї, але якщо розглянути її мінор (наприклад, $A_{NN}$), то отримаємо гарну
матрицю, яка має визначник і зворотню матрицю. Ми просто не розглядаємо
гістограму $h_N^k$. Тоді обернена до мінора матриця буде виглядати наступним
чином
\begin{equation*}
  A_{NN}^{-1} = \frac{1}{p} \cdot
    \begin{bmatrix}
      \frac{1}{\rho_1^k}+\frac{1}{p_N^k} & \frac{1}{\rho_N^k}
        & \cdots & \frac{1}{\rho_N^k}                 \\
      \frac{1}{\rho_N^k}                 & \frac{1}{\rho_2^k}+\frac{1}{p_N^k}
        & \cdots & \frac{1}{\rho_N^k}                 \\
      \vdots                             & \vdots
        & \ddots & \vdots                             \\
      \frac{1}{\rho_N^k}                 & \frac{1}{\rho_N^k}
        & \cdots & \frac{1}{\rho_{N-1}^k}+\frac{1}{p_N^k}
    \end{bmatrix}
\end{equation*}
Якщо розглянути щільність отриманого гаусівського розподілу, то показник
експоненти буде
\begin{equation*}
  R^k
  = \left( S^k - \mean{S^k} \right) \cdot A_{NN}^{-1}
      \cdot \left( S^k - \mean{S^k} \right)^T
  = p \cdot \sum_{i=1}^{N}\frac{\left( h_i^k - \rho_i^k \right)^2}{\rho_i^k}
\end{equation*}
\begin{comment}
Маємо суму $p$ квадратів \textbf{залежних} між собою випадкових величин, що
приблизно мають стандартний гаусовський розподіл.
Хотілося б мати розподіл суми квадратів \textbf{незалежних} стандартних
гаусовських випадкових величин --- розподіл Пірсона $\chi^2$.
Виявляється, що залежність отриманих випадкових величин
забирає лише один ступінь вільності, і на виході отримуємо розподіл Пірсона з
$N-1$ ступенями вільності \cite{Hudson:1963}
\end{comment}
Аналіз характеристичної функції величини $R^k$ показує, що її розподіл тим
більше прямує до розподілу $\chi^2$ з $N-1$ ступенями вільності, чим більше
розмір аналізованої вибірки $p$
\begin{equation*}
  R^k \Covergencen{p} \chi_{N-1}^2
\end{equation*}

З таблиці для функції розподілу $\chi_{N-1}^2$ обираємо рівень значущості
$\alpha$ і шукаємо відповідне до кількості ступенів вільності $r_{\alpha}$.
Рівень значущості --- ймовірність помилки першого роду, тобто ймовірність того,
що буде відкинуто вірну гіпотезу
\begin{equation*}
  \probability{\chi_{N-1}^2 \ge r_{\alpha}} = \alpha
\end{equation*}
Якщо $R^k \le r_{\alpha}$, то гіпотеза про те, що вибірка $Y^k$ дійсно має
розподіл $F^k$, не відхиляється. Інакше $R^k$ буде поводитись як $\sqrt{p}$
і достатньо швидко зростати при великих $p$, а гіпотезу буде відхилено.

Чим більше рівень значущості, тим менше значення $r_{\alpha}$, а отже і
проміжок, в який дозволяється потрапити значенню $R^k$.
Тобто, більша ймовірність відхилити вірну гіпотезу щодо розподілу,
але при цьому є більше впевненості в правильності результату.
Зазвичай $\alpha$ обирають рівним $0.1$, $0.05$, $0.001$.
