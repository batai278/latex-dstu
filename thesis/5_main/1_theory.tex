\section{Теоретичні відомості}
\subsection{Метод головних компонент}
Метод головних компонент (Principal component analysis) --- метод, що дозволяє
зменшити розмірність досліджуваної вибірки з мінімальними втратами інформації.

Маємо $m$ об’єктів, з яких треба зняти по $n$ певних властивостей.
На вході в нас є виборки $\vec{X}_k$, кожна з яких відповідає сукупності
властивостей $k$-го об’єкту
\begin{equation*}
  \vec{X}_k = \begin{bmatrix}
    x_k^1  \\
    x_k^2  \\
    \vdots \\
    x_k^n
  \end{bmatrix},
  \qquad k = \overline{1,m}
\end{equation*}
Згрупуємо всі вимірювання в одну матрицю $X$
\begin{equation*}
  X = \begin{bmatrix}
    x_1^1  & x_2^1  & \dots  & x_m^1  \\
    x_1^2  & x_2^2  & \dots  & x_m^2  \\
    \vdots & \vdots & \ddots & \vdots \\
    x_1^n  & x_2^n  & \dots  & x_m^n
  \end{bmatrix}
\end{equation*}

Спочатку нам знадобиться знайти вибіркові середні значення для кожної
властивості
\begin{equation*}
  a_i = \frac{1}{m} \cdot \sum_{k=1}^{m} x_k^i, \qquad i = \overline{1,n}
\end{equation*}
Маємо вектор вибіркових середніх значень
\begin{comment}
\begin{equation*}
  \vec{a} = \begin{bmatrix}
    \frac{1}{m} \sum_{k=1}^{m} x_k^1 \\
    \frac{1}{m} \sum_{k=1}^{m} x_k^2 \\
    \vdots                           \\
    \frac{1}{m} \sum_{k=1}^{m} x_k^n \\
  \end{bmatrix}
\end{equation*}
\end{comment}
\begin{equation*}
  \vec{a} = \begin{bmatrix}
    a_1    \\
    a_2    \\
    \vdots \\
    a_n
  \end{bmatrix}
\end{equation*}

Центруємо отримані дані, що містяться в матриці $X$, віднявши від кожного
стовбця вектор вибіркових середніх $\vec{a}$
\begin{comment}
\begin{equation*}
  \tilde{x}_k^i = x_k^i - a_i,\qquad k = \overline{1,m}, i = \overline{1,n}
\end{equation*}
Кожен стовбець матриці вимірювань прийме вигляд
\begin{equation*}
  \tilde{X}_k
  = \vec{X}_k - \vec{a}
  = \begin{bmatrix}
    x_k^1 - a_1 \\
    x_k^2 - a_2 \\
    \vdots      \\
    x_k^n - a_n
  \end{bmatrix},
  \qquad k = \overline{1,m}
\end{equation*}
Маємо матрицю центрованих вимірювань $\tilde{X}$
\end{comment}
\begin{equation*}
  \tilde{X}
  = \begin{bmatrix}
    \tilde{x}_1^1  & \tilde{x}_2^1  & \dots  & \tilde{x}_m^1  \\
    \tilde{x}_1^2  & \tilde{x}_2^2  & \dots  & \tilde{x}_m^2  \\
    \vdots & \vdots & \ddots & \vdots \\
    \tilde{x}_1^n  & \tilde{x}_2^n  & \dots  & \tilde{x}_m^n
  \end{bmatrix}
  = \begin{bmatrix}
    x_1^1 - a_1  & x_2^1 - a_1  & \dots  & x_m^1 - a_1 \\
    x_1^2 - a_2  & x_2^2 - a_2  & \dots  & x_m^2 - a_2 \\
    \vdots       & \vdots       & \ddots & \vdots      \\
    x_1^n - a_n  & x_2^n - a_n  & \dots  & x_m^n - a_n
  \end{bmatrix}
\end{equation*}

Обчислюємо вибіркову коваріаційну матрицю властивостей.
Вибіркову коваріацію $i$ та $j$ властивості рахуємо за формулою
\begin{equation*}
  \sigma_i^j
  = \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^i \cdot \tilde{x}_k^j
  = \frac{1}{m} \cdot \sum_{k=1}^{m}
    \left[ \left( x_k^i - a_i \right) \cdot \left( x_k^j - a_j \right) \right],
    \qquad i,j = \overline{1,n}
\end{equation*}
Маємо вибіркову коваріаційну матрицю
\begin{equation*}
  K = \begin{bmatrix}
    \sigma_1^1 & \sigma_2^1 & \dots  & \sigma_n^1 \\
    \sigma_1^2 & \sigma_2^2 & \dots  & \sigma_n^2 \\
    \vdots     & \vdots     & \ddots & \vdots     \\
    \sigma_1^n & \sigma_2^n & \dots  & \sigma_n^n \\
  \end{bmatrix}
\end{equation*}
\begin{comment}
\begin{equation*}
  K =
  \begin{bmatrix}
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^1 \cdot \tilde{x}_k^1
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^1 \cdot \tilde{x}_k^2
    &
    \dots
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^1 \cdot \tilde{x}_k^n
    \\
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^2 \cdot \tilde{x}_k^1
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^2 \cdot \tilde{x}_k^2
    &
    \dots
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^2 \cdot \tilde{x}_k^n
    \\
    \vdots & \vdots & \dots & \vdots \\
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^n \cdot \tilde{x}_k^1
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^n \cdot \tilde{x}_k^2
    &
    \dots
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^n \cdot \tilde{x}_k^n
  \end{bmatrix}
\end{equation*}
\end{comment}

Щоб отримувати лише потрібну інформацію, ми хочемо знайти таке ортогональне
лінійне перетворення $L$ вхідної матриці $\tilde{X}$, щоб отримати матрицю
$Y = L \cdot \tilde{X}$, яка має діагональну вибіркову ковариаційну матрицю $K'$
з незростаючими зверху вниз значеннями.
Діагональна вибіркова коваріаційна матриця гарантує той факт, що отримані
значення $Y$ будуть некорельованими.
Рангування значень діагональних елементів матриці $K'$ за величиною дасть більш
наглядне представлення про будову досліджуваних об’єктів, адже діагональні
елементи --- вибіркові дисперсії; а чим більше дисперсія, тим більше відповідна
властивість змінюється від об’єкту до об’єкту і тим більше корисної інформації
вона нам надає.

Вибіркова коваріаційна матриця $K'$ для $Y = L \cdot \tilde{X}$ має вигляд
\begin{equation*}
  K'
  = L \cdot K \cdot L^*
  = \begin{bmatrix}
    \lambda_1 & 0         & \dots  & 0      \\
    0         & \lambda_2 & \dots  & 0      \\
    \vdots    & \vdots    & \ddots & \vdots \\
    0         & 0         & \dots  & \lambda_n
  \end{bmatrix}
\end{equation*}
З лінійної алгебри відомо, що матриця $L$ складається з координат власних
векторів матриці $K$, а елементи $\lambda_k$ --- її власні числа, які
існують і є невід’ємними через невід’ємну означеність матриці $K$.
Вважаємо що числа $\lambda_1, \dots, \lambda_n$ впорядковані від більшого до
меншого для зручності подальших дій.
Позначимо власний вектор матриці $K$, що відповідає власному числу $\lambda_k$,
як $\vec{l}_k$. Тоді
\begin{equation*}
  \vec{l}_k
  = \left[ l_k^1, l_k^2, \dots, l_k^n \right],
  \qquad k = \overline{1,n}
\end{equation*}
Матриця $L$ має вигляд
\begin{equation*}
  L = \begin{bmatrix}
    l_1^1  & l_1^2  & \dots  & l_1^n  \\
    l_2^1  & l_2^2  & \dots  & l_2^n  \\
    \vdots & \vdots & \ddots & \vdots \\
    l_n^1  & l_n^2  & \dots  & l_n^n  \\
  \end{bmatrix}
\end{equation*}

Треба зменшити розмірність простору досліджуваних параметрів системи з $n$ до
$p<n$, але при цьому втратити якомога менше відомостей про досліджувані
об’єкти.
Введемо міру інформації, що залишається при зменшенні кількості компонент, що
розглядаються
\begin{equation*}
  I = \frac{\lambda_1 + \dots + \lambda_p}{\lambda_1 + \dots + \lambda_n}
\end{equation*}
Будемо вважати, що діємо продуктивно, тому починаємо обирати з перших
компонент, адже саме вони є найбільш інформативними.
Також бачимо, що інформативність змінюється в межах від $0$
(нічого не дізнаємось) до $1$ (зберегли усю інформацію).

Надалі буде розглядатися матриця головних компонент $Y$
\begin{equation*}
  Y = \begin{bmatrix}
    y_1^1  & y_2^1  & \dots  & y_m^1  \\
    y_1^2  & y_2^2  & \dots  & y_m^2  \\
    \vdots & \vdots & \ddots & \vdots \\
    y_1^p  & y_2^p  & \dots  & y_m^p  \\
    \end{bmatrix}
\end{equation*}
\subsection{Гістограма}

Для подальшого аналізу потрібно здобути щільність розподілу головних компонент.
Оскільки маємо справу з вибіркою і вибірковими характеристиками,
потрібно побудувати гістограму, адже це і є вибіркова характеристика,
що відповідає щільності.

Побудуємо $j$-й стовбець гістограми для виборки з $k$-ї строки матриці $Y$
\begin{equation*}
  h_j^k = \frac{1}{m} \cdot \sum_{i=1}^{m} \indicator{y_i^k \in I_j^k},
  \qquad j = \overline{1, N},
  \qquad k = \overline{1, p}
\end{equation*}
де $I^k$ --- набір напівінтервалів, що розбиває відрізок
$\left[ \min\limits_{i=\overline{1,m}}{y_i^k};
\max\limits_{i=\overline{1,m}}{y_i^k} \right]$ на $N$ рівних частин.
Для вибору $N$ можна скористатися досить відомою формулою Стьорджеса
(Sturges' formula) \cite{Sturges:1926:CCI}
\begin{equation*}
  N = \lfloor \log_2 m \rfloor + 1
\end{equation*}
Маємо матрицю гістограм
\begin{equation*}
  H = \begin{bmatrix}
    h_1^1  & h_2^1  & \dots  & h_N^1  \\
    h_1^2  & h_2^2  & \dots  & h_N^2  \\
    \vdots & \vdots & \ddots & \vdots \\
    h_1^p  & h_2^p  & \dots  & h_N^p
  \end{bmatrix}
\end{equation*}
і напівінтервалів, що відповідають кожному стовбчику кожної гістограми
\begin{equation*}
  I = \begin{bmatrix}
    I_1^1  & I_2^1  & \dots  & I_N^1  \\
    I_1^2  & I_2^2  & \dots  & I_N^2  \\
    \vdots & \vdots & \ddots & \vdots \\
    I_1^p  & I_2^p  & \dots  & I_N^p
  \end{bmatrix}
\end{equation*}

\subsection{Критерій узгодженості Пірсона $\chi^2$}
Гістограма може використовуватися не тільки для графічної інтерпретації
отриманих даних, але й для віднесення вибірки до якогось відомого розподілу.
Відповідь на питання ``Чи дійсно вибірка $y_1^k$, $\dots$, $y_m^k$ має розподіл
$F^k$?'' може надати критерій узгодженості Пірсона.

Розглянемо вектор
\begin{equation*}
    \eta^k
    = \left[ \frac{\nu_1^k - m \cdot \rho_1^k}{\sqrt{m \cdot \rho_1^k}}, \dots,
      \frac{\nu_N^k - m \cdot \rho_N^k}{\sqrt{m \cdot \rho_N^k}} \right]
\end{equation*}
Знайдемо його характеристичну функцію
\begin{equation*}
  \varphi_{\eta^k}\left( \lambda\right)
  = \mean{e^{i \cdot \left( \lambda, \eta^k \right)}},\qquad
  \lambda \in \mathbb{R}^N
\end{equation*}
Для зручності перепозначимо індикатор
\begin{equation*}
  \mathfrak{I}_{i,j}^k = \Indicator{y_i^k \in I_j^k}
\end{equation*}
Подивимось, чому дорівнює скалярний добуток в експоненті
\begin{equation*}
  \begin{split}
    \left( \lambda, \eta^k \right)
    &= \sum_{j=1}^{N} \lambda_j \cdot \frac{\nu_j^k - m \cdot \rho_j^k}{
      \sqrt{m \cdot \rho_j^k}}
    = \sum_{j=1}^{N}\frac{\lambda_j}{\sqrt{m \cdot \rho_j^k}}
      \cdot \sum_{i=1}^{m}\left( \mathfrak{I}_{i,j}^k - \rho_j^k \right) = \\
    &= \sum_{j=1}^{N} \sum_{i=1}^{m}
      \frac{\lambda_j}{\sqrt{m \cdot \rho_j^k}}
      \cdot \left( \mathfrak{I}_{i,j}^k - \rho_j^k \right)
    = \sum_{i=1}^{m} \sum_{j=1}^{N} \lambda_j \cdot
      \frac{\mathfrak{I}_{i,j}^k - \rho_j^k}{\sqrt{m \cdot \rho_j^k}}
  \end{split}
\end{equation*}
Бачимо суму $m$ незалежних однаково розподілених випадкових величин.
Введемо позначення
\begin{equation*}
  \mathfrak{I}_j^k = \Indicator{y_1^k \in I_j^k}
\end{equation*}
А також позначимо новий випадковий вектор
\begin{equation*}
  \zeta^k
  = \left[ \frac{\mathfrak{I}_1^k - \rho_1^k}{\sqrt{m \cdot \rho_1^k}},
    \dots
    \frac{\mathfrak{I}_N^k - \rho_N^k}{\sqrt{m \cdot \rho_N^k}} \right]
\end{equation*}
Тоді скалярний добуток прийме вигляд
\begin{equation*}
    \left( \lambda, \eta^k \right)
    = \sum_{i=1}^{m} \sum_{j=1}^{N} \lambda_j \cdot \zeta_j^k
    = \sum_{i=1}^{m} \left( \lambda, \zeta^k \right)
    = m \cdot \left( \lambda, \zeta^k \right)
\end{equation*}
За рахунок незалежності випадкових величин $\zeta_j^k$ маємо
\begin{equation}\label{eq:cf:eta}
  \varphi_{\eta^k}\left( \lambda\right)
  = \mean{e^{i \cdot \left( \lambda, \eta^k \right)}}
  = \mean{e^{m \cdot i \cdot \left( \lambda, \zeta^k \right)}}
  = \left( \mean{e^{i \cdot \left( \lambda, \zeta^k \right)}} \right)^m
\end{equation}
Розглянемо характеристичну функцію випадкового вектора $\zeta^k$
\begin{equation}\label{eq:cf:zeta}
  \varphi_{\zeta^k}\left( \lambda \right)
  = \Mean{\exp{\left\{ i
    \cdot \sum_{j=1}^{N} \lambda_j \cdot \zeta_j^k \right\}}}
\end{equation}
Легко побачити, що
\begin{equation*}
  \begin{split}
    \left( \lambda, \zeta^k \right)
    = \sum_{j=1}^{N} \lambda_j \cdot \zeta_j^k
    = \sum_{j=1}^{N} \lambda_j
      \cdot \frac{\mathfrak{I}_j^k - \rho_j^k}{\sqrt{m \cdot \rho_j^k}}
    &= \sum_{j=1}^{N} \left(
       \frac{\lambda_j}{\sqrt{m \cdot \rho_j^k}} \cdot \mathfrak{I}_j^k
       - \frac{\sqrt{\rho_j^k} \cdot \lambda_j}{\sqrt{m}} \right) = \\
    &= \sum_{j=1}^{N} \mathfrak{I}_j^k \cdot \left( 
        \frac{\lambda_j}{\sqrt{m \cdot \rho_j^k}}
        - \sum_{l=1}^{N} \frac{\sqrt{\rho_l^k} \cdot \lambda_l}{\sqrt{m}}
      \right)
  \end{split}
\end{equation*}
Тобто характеристична функція \eqref{eq:cf:zeta} приймає вигляд
\begin{equation*}
  \varphi_{\zeta^k}\left( \lambda \right)
  = \Mean{\sum_{j=1}^{N} \mathfrak{I}_j^k
    \cdot \exp{\left\{ \frac{i}{\sqrt{m}} \left(
      \frac{\lambda_j}{\sqrt{\rho_j^k}}
      - \sum_{l=1}^{N} \sqrt{\rho_l^k} \cdot \lambda_l \right) \right\}}}
\end{equation*}
Перепозначимо вираз в круглих дужках
\begin{equation*}
  \mathfrak{z}^k
  = \frac{\lambda_j}{\sqrt{\rho_j^k}}
    - \sum_{l=1}^{N} \sqrt{\rho_l^k} \cdot \lambda_l
\end{equation*}
Математичне очікування індикатора --- ймовірність події, яку він перевіряє.
Отже
\begin{equation*}
  \varphi_{\zeta^k}\left( \lambda \right)
  = \sum_{j=1}^{N} \rho_j^k
    \cdot \exp{\left\{ \frac{i \cdot \mathfrak{z}^k}{\sqrt{m}} \right\}}
\end{equation*}

Якщо розмір вибірки $m$ буде зростати, то характеристична функція $\eta^k$
\eqref{eq:cf:eta} буде поводитись наступним чином
\begin{equation*}
  \begin{split}
    \lim_{m \to \infty} \varphi_{\eta^k}\left( \lambda \right)
    &= \lim_{m \to \infty} \left( 1 + \sum_{k=1}^{N} p_k \cdot \left[
      \exp{\left\{ \frac{i \cdot \mathfrak{z}^k}{\sqrt{m}} \right\}}
      - 1 \right] \cdot \frac{m}{m} \right)^m = \\
    &= \lim_{m \to \infty} \exp{ \left\{ m \cdot \sum_{k=1}^{N} p_k \cdot \left[
      \exp{\left\{ \frac{i \cdot \mathfrak{z}^k}{\sqrt{m}} \right\}}
      - 1 \right] \right\}}
  \end{split}
\end{equation*}
Для $\exp{\left\{ \frac{i \cdot \mathfrak{z}^k}{\sqrt{m}} \right\}}$
використаємо співвідношення
\begin{equation*}
  e^{\alpha} - 1 \approx \alpha + \frac{\alpha^2}{2},\qquad \alpha \ll 1
\end{equation*}
Маємо
\begin{equation*}
  \begin{split}
    \sum_{j=1}^{N} \rho_j^k \cdot \frac{i \cdot \mathfrak{z}^k}{\sqrt{m}}
    &= \sum_{j=1}^{N} \rho_j^k \cdot \frac{i}{\sqrt{m}}
      \cdot \left( \frac{\lambda_j}{\sqrt{\rho_j^k}}
        - \sum_{l=1}^{N} \sqrt{\rho_l^k} \cdot \lambda_l \right) = \\
    &= \frac{i}{\sqrt{m}} \cdot \left(
      \sum_{j=1}^{N} \sqrt{\rho_j^k} \cdot \lambda_j
      - \sum_{l=1}^{N} \sqrt{\rho_l^k} \cdot \lambda_l \right)
    = 0,
  \end{split}
\end{equation*}
\begin{equation*}
  \begin{split}
    \sum_{j=1}^{N} \rho_j^k
      \cdot \left( \frac{i \cdot \mathfrak{z}^k}{\sqrt{m}} \right)^2
      = - \sum_{j=1}^{N} \frac{\rho_j^k}{m}
        \cdot \left( \frac{\lambda_j}{\sqrt{\rho_j^k}}
          - \sum_{l=1}^{N} \sqrt{\rho_l^k} \cdot \lambda_l \right)^2 = \\
      = - \frac{1}{m} \cdot \left[ \sum_{j=1}^{N} \lambda_j
        - \left( \sum_{l=1}^{N} \sqrt{\rho_l^k} \cdot \lambda_l \right)^2
          \right]
  \end{split}
\end{equation*}
Тому
\begin{equation*}
  \begin{split}
    \lim_{m \to \infty} \varphi_{\eta^k}\left( \lambda \right)
    &= \lim_{m \to \infty} \exp{\left\{ -\frac{m}{m \cdot 2}
      \cdot \left[ \sum_{j=1}^{N} \lambda_j
        - \left( \sum_{l=1}^{N} \sqrt{\rho_l^k} \cdot \lambda_l \right)^2
        \right]\right\}} = \\
    &= \exp{\left\{ -\frac{1}{2} \cdot
      \left[ \sum_{j=1}^{N} \lambda_j
        - \left( \sum_{l=1}^{N} \sqrt{\rho_l^k} \cdot \lambda_l \right)^2
        \right] \right\}}
    = e^{-\frac{1}{2} \cdot \left( A^k \lambda, \lambda \right)}
  \end{split}
\end{equation*}
Матриця $A^k$ побудована наступним чиином
\begin{equation*}
  A^k
  = \left\| \delta_{ij} - \sqrt{\rho_i^k} \cdot \sqrt{\rho_j^k} \right\|_{i=1}^n
\end{equation*}
Симетричність мариці очевидна, тому треба довести її невід’ємну визначеність,
щоб стверджувати, що вона є коваріаційною.
Для цього візьмемо вектор
\begin{equation*}
  e^k = \left[ \sqrt{\rho_1^k}, \dots, \sqrt{\rho_N^k} \right],\qquad
  \left\| e^k \right\| = 1
\end{equation*}
Тоді бачимо, що
\begin{equation}\label{eq:chi2:quadraticForm}
  \left( A^k \lambda, \lambda \right)
  = \left\| \lambda \right\|^2
    - \left( \lambda, e^k \right)^2
\end{equation}
З нерівності Коші маємо
\begin{equation*}
  \left\| \left( \lambda, e^k \right) \right\|
  \le \left\| \lambda \right\| \cdot \left\| e^k \right\|
  = \left\| \lambda \right\|
\end{equation*}
Тобто матриця є дійсно невід’ємно визначеною і вектор $\eta^k$ розподілений
за нормальним законом з нульовим середнім і коваріаційною матрицею $A^k$.

Для подальших розрахунків розглянемо стандартний гаусівський вектор
як суму випадкових нормально розподілених випадкових величин в стандартному
базисі $\mathbb{R}^N$, який позначимо $\left[ e_1, \dots, e_N \right]$
\begin{equation*}
  \xi = \sum_{j=1}^{N}\xi_j \cdot e_j \sim N\left( \vec{0}, I \right)
\end{equation*}
Згадуємо, що ортогональні перетворення $U$ зберігають відстані, а також
справедливо наступне
\begin{equation*}
  U\xi \sim N\left( 0, U I U^{-1} \right) \sim N\left( \vec{0}, I \right)
\end{equation*}
Також ортонормований базис залишається ортонормованим базисом після
ортогонального перетворення $U$.
Оберемо такий оператор $U$, щоб набір $\left[ e_1, \dots, e_N \right]$ під його
дією перетворився на $\left[ f_1, \dots, f_N \right]$, де
\begin{equation*}
  f_1 = e^k = \left[ \sqrt{\rho_1^k}, \dots, \sqrt{\rho_N^k} \right]
\end{equation*}
Тоді маємо вектор
\begin{equation*}
  U\xi
  = \hat{\xi}
  = \sum_{j=1}^{N} \hat{\xi_j} \cdot f_j \sim N\left( \vec{0}, I \right)
\end{equation*}
Подивимось, який розподіл має наступний вектор
\begin{equation*}
  \Upsilon
  = \sum_{j=2}^{N} \hat{\xi_j} \cdot f_j
  = \hat{\xi} - \hat{\xi_1} \cdot e^k
\end{equation*}
Для цього розглянемо квадратичну форму
\begin{equation*}
  \mean{\left( \Upsilon, \lambda \right)^2}
  = \sum_{j=2}^{N} \left( \lambda, f_j \right)^2
  = \sum_{j=1}^{N} \left( \lambda, f_j \right)^2 - \left( \lambda, f_1 \right)^2
  = \left\| \lambda \right\|^2 - \left( \lambda, e^k \right)^2
  = \left( A^k \lambda, \lambda \right)
\end{equation*}
З рівності \eqref{eq:chi2:quadraticForm} бачимо, що випадкові вектори $\eta^k$
і $\Upsilon$ мають однаковий розподіл.
Отже, розподіли їх норм теж співпадають.
Оскільки сума $N-1$ квадратів незалежних стандартних гаусових випадкових
величин має розподіл Пірсона з $N-1$ ступенями вільності
\begin{equation*}
  \left\| \Upsilon \right\|^2 = \sum_{j=2}^{N} \xi_j^2 \sim \chi_{N-1}^2
\end{equation*}
Маємо
\begin{equation*}
  \left\| \eta^k \right\|
  = \sum_{j=1}^{N}\frac{\left( \nu_j^k - m \cdot \rho_j^k \right)^2}{
    m \cdot \rho_j^k}
  = m \cdot \sum_{j=1}^{N}\frac{\left( h_j^k - \rho_j^k \right)^2}{\rho_j^k}
  \sim \chi_{N-1}^2
\end{equation*}

Останнє співвідношення дає змогу перевіряти належність виборки
$y_1^k$, $\dots$, $y_m^k$ до розподілу $F^k$.
Перевірка виглядає наступним чином.

Розглянемо випадкову величину
\begin{equation}\label{eq:pearson:Rk}
  R^k
  = m \cdot \sum_{j=1}^{N}\frac{\left( h_j^k - \rho_j^k \right)^2}{\rho_j^k}
\end{equation}

Обираємо рівень значущості $\alpha$ для функції розподілу $\chi_{N-1}^2$ і
шукаємо відповідне до кількості ступенів вільності $r_{\alpha}$.
Рівень значущості --- ймовірність помилки першого роду, тобто ймовірність того,
що буде відкинуто вірну гіпотезу
\begin{equation*}
  \probability{\chi_{N-1}^2 \ge r_{\alpha}} = \alpha
\end{equation*}
Якщо $R^k \le r_{\alpha}$, то гіпотеза про те, що вибірка $Y^k$ дійсно має
розподіл $F^k$, приймається.

Розглянемо той випадок, коли ймовірність $\rho_i^k$
відгадана невірно. Повернемося до формули \eqref{eq:pearson:Rk}
\begin{equation*}
  R^k
  = \sum_{j=1}^{N}\frac{\left( \nu_j^k - m \cdot \rho_j^k \right)^2}{m \cdot \rho_j^k}
\end{equation*}
Всі члени суми є невід’ємними. Якщо хоча б один елемент буде завеликим,
то великою буде вся сума. Маємо випадкову величину $\eta$
\begin{equation*}
  \eta
  = \nu_i^k - m \cdot \rho_i^k
  = \sum_{j=1}^{m} \left( \xi_j - \rho_i^k \right),
  \qquad \indicator{y_j^k \in I_i^k} = \xi_j
\end{equation*}
Якщо $\rho_i^k$ вгадано невірно, то воно не дорівнює математичному очікуванню
індикатора. Додамо та віднімемо справжнє математичне очікування
\begin{equation*}
  \eta
  = \sum_{j=1}^{m} \left( \xi_j - \mean{\xi_1} + \mean{\xi_1} - \rho_i^k \right)
  = \sum_{j=1}^{m} \left( \xi_j - \mean{\xi_1} \right)
    + \sum_{j=1}^{m} \left( \mean{\xi_1} - \rho_i^k \right)
\end{equation*}
Останній доданок є просто різницею, помноженою на $m$
\begin{equation*}
  \eta
  = \sum_{j=1}^{m} \left( \xi_j - \mean{\xi_1} \right)
    + m \cdot \left( \mean{\xi_1} - \rho_i^k \right)
\end{equation*}
Поділимо на $\sqrt{m}$, щоб скористатися центральною граничною теоремою
\begin{equation*}
  \frac{\eta}{\sqrt{m}}
  = \frac{1}{\sqrt{m}} \cdot \sum_{j=1}^{m} \left( \xi_j - \mean{\xi_1} \right)
    + \frac{1}{\sqrt{m}} \cdot m \cdot \left( \mean{\xi_1} - \rho_i^k \right)
\end{equation*}
Перший доданок має розподіл $N\left( 0, \sigma^2 \right)$, де $\sigma^2$ ---
дисперсія випадкової величини $\xi_1$ для достатньо великих $m$.
Отже, вся сума зростає пропорційно
$\sqrt{m}$
\begin{equation*}
  \frac{\eta}{\sqrt{m}}
  = \frac{1}{\sqrt{m}} \cdot \sum_{j=1}^{m} \left( \xi_j - \mean{\xi_1} \right)
    + \sqrt{m} \cdot \left( \mean{\xi_1} - \rho_i^k \right)
  \sim \sqrt{m} \cdot \left( \mean{\xi_1} - \rho_i^k \right)
\end{equation*}
Тобто якщо гіпотеза невірна, то $R^k$ буде зростати пропорційно до величини
$m$, що є достатньо великою швидкістю.

Чим більше рівень значущості, тим менше значення $r_{\alpha}$, а отже і
проміжок, в який дозволяється потрапити значенню $R^k$.
Зазвичай $\alpha$ обирають рівним $0.1$, $0.05$, $0.01$.

\subsection{Типи вищої нервової діяльності}

Тепер потрібно визначити, які показники отримувати і яким чином це робити.

Почнемо з типів вищої нервової діяльності (ВНД).
Згідно з Павловим \cite{Pavlov:1923} ці типи характеризуються трьома показниками
--- сила нервової системи (сильна або слабка), врівноваженість (врівноважена або
неврівноважена) та рухливість (рухлива або інетртна).
Павлов розглядає 4 комбінації цих показників з 8 можливих:
\begin{enumerate}
  \item Слабка
  \item Сильна та неврівноважена
  \item Сильна, врівноважена та інертна
  \item Сильна, врівноважена та рухлива
\end{enumerate}
Далі ці класи (комбінації) будуть називатися відповідно слабкий,
неврівноважений, інертний та рухливий.
Щоб віднести людину до того чи іншого класу, існує тест Айзенка.
Складність полягає в тому, що не дуже часто зустрічаються чисті типи ---
зазвичай вони змішані.
Це стає на заваді наступному етапу --- моделюванню.

\subsection{Теппінг-тест}

Існують відомі залежності між типом вищої нервової діяльності та зміною
максимального темпу рухів кистю руки з часом.
Протягом 30 секунд людина намагається притримуватися максимально можливого для
себе темпу.
Показники темпу фіксуються через кожні 5 секунд, а далі по 6 отриманим точкам
будується крива темпа руху. \cite{Ilin:2001}

Для тесту можна використовувати ручку (олівець) і папір, або телеграф.
Сучасні технології дозволяють проводити тест за допомогою клавіатури комп’ютера
або екрану планшета.

З олівцем і папіром тест поводиться наступним чином:
\begin{enumerate}
  \item На папері кресляться 6 квадратів
  \item Людина починає ставити якомога більше точок в першому квадраті впродовж
    перших 5 секунд
  \item Коли проходить 5 секунд, потрібно перейти до наступного квадрату і
    ставити точки там
  \item Процедура повторюється до тих пір, доки не пройде 30 секунд --- в кінці
    буде заповнено всі 6 квадратів
\end{enumerate}
Далі підраховується кількість точок в кожному квадраті та малюється ламана ----
горизонтальна вісь відповідає номеру часового проміжку (номеру квадрата), а
вертикальна відповідає кількості точок в квадраті.

Трактуються отримані дані наступним чином:
\begin{enumerate}
  \item Спадна ламана відповідає слабкому типу.
    Вона спадає після перших 5 секунд тесту і не повертається до початкового
    рівня
  \item Проміжна між рівною та опуклою ламана відповідає неврівноваженому типу.
    Така ламана зростає в перші 10-15 секунд, а потім спадає
  \item Вігнута ламана відповідає інертному типу.
    Вона спочатку спадає, а на 25-30 секундах може зрости до початкового темпу
  \item Опукла ламана відповідає рухливому типу.
    Це така ламана, що зростає в перші 10-15 секунд тесту, а до 25-30 секунд
    може впасти нижче початкового темпу
\end{enumerate}
Також темп може залишатися приблизно на одному рівні протягом усього тесту.
